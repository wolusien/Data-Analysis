---
title: 'TP2 : Régression Linéaire'
author: "DJEBALI Wissam"
date: "1 mars 2018"
output:
  pdf_document: default
  word_document: default
header-includes: 
 - \usepackage[french]{babel}
 - \usepackage[utf8]{inputenc}
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(factoextra)
library(FactoMineR)
library(corrplot)
library(stats)
```

## Linear Regression

**Packages R** : FactoMineR, factoextra, corrplot, stats, MASS

###Simple Linear Regression with prevision
La régression lineaire simple est une généralisation du test de corrélation et du test t
```{r}
data(cats)
# Pour avoir une idee des possible correlation entre les variables
plot(cats)

# Retrait d'un échantillon de donnees servant de test
# Tirage aleatoire d'indice a retirer
ech =sample(1:nrow(cats),10)
# Extraction des indices
popu = cats[-ech,]
# ou
# Extraire des donnees d'une data
# Ici on extrait toutes les lignes ou l'espèces n'est pas 'Iris_setosaa'
# data2<- subset(data, data$species!='Iris-setosa')


# HWt en fonction de Bwt
plot(Bwt~Hwt,data=popu) # ou plot(Bwt,Hwt,data=popu)

# meilleur affichage
# augermet le bruit avec factor
plot(jitter(popu$Bwt),jitter(popu$Hwt, factor=4))

# Corrélation entre Bwt et Hwt
cor(popu$Bwt,popu$Hwt)

# Régression linéaire
reg<-lm(popu$Hwt~popu$Bwt) # Y~Var1+Var2+Var3
summary(reg)
```

**(Intercept)**  1.038601* = **a** = *coefficient directeur de la droite*

**cats$Hwt**    0.159450 = **b** = *ordonnée à l'origine*   

**Std. Error** : *De petites valeurs sont un gage de stabilité du modèle donc du pouvoir prédictif*:(Ici valeur pour b de 0.72 et pour a de 0.26 très stable)

**Residual standard error**:  *Écart-type résiduel doit être faible pour bon pouvoir prédictif* (Ici 1.458 ce qui est faible)

**Multiple R-squared** *% de la variance de Hwt expliquée par Bwt, doit être proche de 1 pour avoir un bon pouvoir explicatif*:(Ici 64%)

**p-value** <2e-16  *probabilité que l'ordonnée à l'origine soit proche de 0*



```{r}
plot(reg)

# les résidus
reg$residuals # ou residuals(reg)

# les valeurs des Y(ici Hwt) ajustées
reg$fitted.values # ou fitted.values(reg)

# Traçage de la droite de régression linéaire
coeff=coefficients(reg)
# Equation de la droite de regression : 
eq = paste0("Y = ", round(coeff[2],3), "*X +", round(coeff[1],3))
# Graphe
plot(Bwt~Hwt,data=popu,main=eq)
abline(reg,col='blue')

# calcul des prédictions
result<-predict(reg,cats[ech,])

# affichage de l'erreur pour chaque estimation
plot((cats[ech,3]-result[ech])/cats[ech,3],col='black')
```

###Multiple Linear Regression with prevision and AIC(Akaike Information Criterion)

```{r}
houses <- read.csv("C:/Users/DJEBALI/Documents/M2_ISIFAR/Data_Mining/houses.txt", sep="")
houses$NE<-as.factor(houses$NE)
houses$Corner<-as.factor(houses$Corner)

# Données test
sampl=sample(1:nrow(houses),10)

houses_sampl=houses[sampl,]

# Description des variables quantitatives
summary(houses)

# Corrélation des variables quantitatives
corrplot(cor(houses[c(-5,-6)]))

# Données d'entraînement
houses_entr = houses[-sampl,]

# Régression lineaire : modéle linéaire gaussien multiple
# regmult<-glm(Price~SQFT+Age+Features+Tax,houses_entr,family=gaussian())
regmult<-lm(Price~SQFT+Age+Features+Tax,houses_entr)
summary(regmult)
```
Comme en régression linéaire simple, les informations données par la fonction summary() concernent :

_ les résidus (maximum, minimum, quartiles)

_ les coefficients : en plus des estimations des $\beta_{j}$ (Estimate), nous avons l’écart-type estimé des estimateurs correspondants (Std. Error), ainsi que la valeur de la statistique de test (t value) et la p-value (Pr(>|t|)) associées aux tests de Student ($H_{0}:\beta_{j}=0$ contre $H_{1}:\beta_{j}\neq0$) correspondants. A noter que des étoiles pour chaque coefficient indiquent le niveau de significativité des différents tests.

_ la qualité d’adéquation du modèle : une estimation de $\sigma$, l’écart-type du terme d’erreur (Residual standard error), la valeur du R2 (Multiple R-squared) et celle du R2 ajusté (Adjusted R-squared), et enfin la valeur de la statistique de test (F-statistic) et la p-value du test de Fisher de significativité du modèle ($H_{0}:\beta_{1}=...=\beta_{p}=0$ contre $H_{1}$ : au moins un $\beta_{j}\neq0$). Ce dernier test permet de tester la nullité simultanée de tous les coefficients associés aux variables explicatives. Ainsi accepter H0 signifie que le modèle proposé n’est pas adéquat.


```{r}
# Graphique des résidus
plot(regmult$fitted.values, regmult$residuals)
abline(h = 0, col = "darkgreen", lwd = 2)

# Graphique des valeurs ajsutées en fonction des valeurs observées
plot(houses_entr$Price, regmult$fitted.values)
abline(a = 0, b = 1, col = "blue", lwd = 2)

# Histogramme des résidus
histo <- hist(regmult$residuals, probability = TRUE)
ec_typ <- summary(regmult)$sigma
curve(dnorm(x, 0, ec_typ), from = min(histo$breaks), to = max(histo$breaks), 
    add = TRUE, type = "l", col = "magenta", lwd = 2)
# On voit qu'on a des résidus gaussiens centrés réduits

# Graphe quantile par quantile
ec_typ <- summary(regmult)$sigma
normed_res <- regmult$residuals/ec_typ
qqnorm(normed_res, xlim = range(normed_res), ylim = range(normed_res))
abline(0, 1, col = "cadetblue", lwd = 2)

# Test de normalité des résidus
shapiro.test(regmult$residuals)
# p<0.05 donc on ne rejette pas l'hypothèse de gaussianité de l'échantillon
```

####AIC(Akaike Information Criterion)

Le critère utilisé par défaut dans R est le critère AIC (pour “An Information Criterion”“, proposé par Akaike, on parle aussi de critère d’Akaike).

La formule du critère AIC (sur lequel la méthodologie de sélection de variables est fondée) est la suivante : $$AIC=n\log(\frac{RSS^{*}}{n})+2(p^{*}+1)$$ où $p^{*}$ correspond au nombre de variables explicatives considérées dans le modèle courant (i.e. celui pour lequel on est en train de calculer l’AIC), et $RSS^{*}=\sum_{i=1}^{n}(y_{i}-y^{*}_{i})^{2}$ est la somme des carrés des résidus du modèle courant (RSS pour Residual Sum of Squares en anglais).

Notons que la quantité RSSn correspond à l’estimation du paramètre $\sigma^{2}$.

Trois types de sélections avec la fonction la step:

_ **Sélection descendante** : *step(regmult, direction='backward')*  
On part du modèle complet puis on enlève à chaque itération la variable la variable qui explique le moins Y.

_ **Sélection ascendante** : *step(regmult, direction='forward')*  
On part du modèle sans covariable et on insère à chaque itération la variable qui explique le plus Y et qui est le plus significative.

_ **Sélection pas à pas (stepwise)** : *step(regmult,direction='both')*  
On part de la méthode ascendante avec remise en cause à chaque étape des variables déjà introduites. Cette pratique permet d'éléminer les variables qui ne sont plus informatives compte tenu de celle qui vient d'être sélectionnée.



```{r}
aic<-step(regmult)
# On garde les variables SQFT et Tax qui expliquent le mieux Price
plot(aic)
aic$residuals
aic$coefficients

```

####Prévisions
```{r}
reg2<-lm(Price~SQFT+Tax,houses_entr)

# Calcul des prédictions
result<-predict(reg2,houses_sampl)
result
plot(result)
houses_sampl$Price
# affichage de l'erreur pour chaque estimation
plot((houses_sampl$Price-result)/houses_sampl$Price,col='black')

```

